require Remote
require math

dim nodes as RemoteNode[]
dim s_node_num as shared int
dim s_thread_per_node as shared int
dim s_barrier as shared RBarrier
dim s_barrier_main as shared RBarrier
dim s_dataset as shared double[] global
dim s_parameters as shared double[] global
dim s_feature_len as shared int
dim s_features as shared int
dim s_iter_num as shared int
dim s_sample_num as shared int

dim node_num as int
dim thread_per_node as int
dim barrier as RBarrier
dim barrier_main as RBarrier
dim dataset as double[] global
dim parameters as double[] global
dim iter_num as int
dim sample_num as int

sub SetNodes(nd as RemoteNode[],thr_per_node as int)
	nodes=nd
	node_num=nd.size()
	s_node_num=node_num
	s_thread_per_node=thr_per_node
	thread_per_node=thr_per_node
end

dim loadpath as shared string
dim barrier_put as shared RBarrier
dim put_target as shared double[] global
function putproc(id as var) as int
	dim mlocal as double[]=new double[s_feature_len*1000]
	dim file as CSVReader=new CSVReader(loadpath)
	dim fl as int=s_feature_len
	dim ds as double[] global=put_target
	dim i as int
	dim j as int
	dim chunk_size as int=s_sample_num/node_num
	file.Skip(chunk_size*id)
	for i=chunk_size*id;i<chunk_size*(id+1);i+=1000
		for j=0;j<1000;j++
			if(file.ReadLine(mlocal,j*fl)<0) then
				file.Reset()
				file.ReadLine(mlocal,j*fl)
			end
		end
		ds[i*fl: (i+1000)*fl]=mlocal[0:fl*1000]
	end
	file.Close()
	barrier_put.Enter(-1)
end

sub LoadDataset(path as string,samples as int,target as double[] global)
	dim tm as int=GetClock()
	dim i as int
	put_target=target
	s_sample_num=samples
	barrier_put=new RBarrier(node_num+1)
	loadpath=path
	for i=0;i<node_num;i++
		nodes[i].CreateThread(putproc,i)
	end
	barrier_put.Enter(-1)
	println("Put time"+(GetClock()-tm))
end


abstract class MLModel
	//public parameters as double[] global
	public abstract  virtual declare sub train(mdataset as double[] global,offset as int,iteration_num as int,samples as int,mstep_size as double) 
	public abstract  virtual declare function predict(datapoint as double[], offset as int) as double
	public abstract  virtual declare function evaluate(mdataset as double[] global,offset as int) as double
end



shared abstract class SGDOptimizer
	public abstract  virtual declare sub optimize(datapoint as double[],offset as int,mgradient as double[],para as double[],mfeatures as int,mstep_size as double) 
end

shared class LogisticRegression:SGDOptimizer
	public override sub optimize(datapoint as double[],base as int,mgradient as double[], para as double[], mfeatures as int,mstep_size as double)
			dim dot as double
			dim j as int
			dim h as double
			for j=0;j<features;j++
				dot+=para[j]*datapoint[base+j]
			end
			h=1/(1+exp(-dot))
			dim delta as double=datapoint[base+features]-h
			for j=0;j<features;j++
				mgradient[j]+= mstep_size * delta * datapoint[base+j]
			end
	end
end

dim sgd_model as shared SGDOptimizer
dim s_minibatch as shared int
dim s_global_gradient as shared double[] global 
dim s_step_size as shared double

dim minibatch as int
dim feature_len as int
dim features as int
dim myparam as double[]
dim gradient as double[][]
dim sync2 as int
dim global_gradient as double[] global 
dim step_size as double

function myrand() as int
	dim p as int=rand(sample_num-10000)
	if p<0 then
		p=-p
	end
	return p
end

function init_SGD(param as var) as int
	minibatch =s_minibatch
	feature_len =s_feature_len
	features =s_features
	myparam=new double[feature_len]
	node_num=s_node_num
	thread_per_node=s_thread_per_node
	gradient=new double[thread_per_node][feature_len]
	barrier=s_barrier
	barrier_main=s_barrier_main
	dataset=s_dataset
	parameters=s_parameters
	global_gradient=s_global_gradient
	iter_num=s_iter_num
	sync2=0
	sample_num=s_sample_num
	barrier_main.Enter(-1)
end
function do_SGD(id as var) as int
	dim model as SGDOptimizer=sgd_model
	dim tid as int=id
	dim i as int
	dim j as int
	dim k as int
	dim st as int=tid % thread_per_node
	dim mround as int
	dim node_id as int=tid/thread_per_node
	dim mydata_set as double[] =new double[feature_len * minibatch]
	dim dot as double
	dim h as double
	dim ran as int
	for mround=0;mround<iter_num;mround++
		//phase 1: copy the parameter vector
		//println("K1 "+mround+" "+st)
		myparam[feature_len*st/thread_per_node : feature_len*(st+1)/thread_per_node]=parameters[feature_len*st/thread_per_node : feature_len*(st+1)/thread_per_node]
		sync2@+=1
		for ;;
			if sync2 % thread_per_node==0 then
				break
			end
		end
		//phase 2: copy the data set
		//println("K2 "+mround+" "+st)
		ran=myrand()
		mydata_set[0:feature_len * 1000]=dataset[ran*feature_len : (ran+1000)*feature_len]
		
		for i=0;i<feature_len;i++
			gradient[st][i]=0
		end

		//phase 3: calculate the gradient on the set of data
		//println("K3 "+mround+" "+st)
		dim base as int
		base=0
		for i=0;i<1000;i++
			model.optimize(mydata_set,base,gradient[st],myparam,features,step_size)
			base+=feature_len
		end

		//phase 4: sum up the local gradient
		//println("K4 "+mround+" "+st)
		if st==0 then
			for i=1;i<thread_per_node;i++
				for j=0;j<features;j++
					gradient[0][j]+=gradient[i][j]
				end
			end
			global_gradient[node_id * feature_len : (node_id+1) * feature_len]=gradient[0][0:feature_len]
		end
		
		barrier_main.Enter(-1)	
		//phase 5: (only on thread 0), sum over the global gradient, calculate the new parameter vector
		//println("K5 "+mround+" "+st)
		if tid==0 then
			mydata_set[0: feature_len * node_num]=global_gradient[0: feature_len * node_num]
			for i=0;i<node_num;i++
				for j=0;j<features;j++
					myparam[j]+=mydata_set[i*feature_len + j]
				end
			end
			parameters[0: feature_len]=myparam[0: feature_len]
		end
		barrier_main.Enter(-1)
	end
	barrier.Enter(-1)
end

class ModelBySGD:MLModel
	public override sub train(ds as double[] global, feature_num as int, iteration_num as int,samples as int,mstep_size as double) 
		s_step_size=mstep_size
		s_dataset=ds
		s_features=feature_num
		s_iter_num=iteration_num
		s_sample_num=samples
		if (feature_num+1) % 32 == 0 then
			s_feature_len = (feature_num+1) / 32 * 32
		else
			s_feature_len = (feature_num+1) / 32 * 32 + 32
		end
		s_global_gradient= new double[feature_len * node_num] global
		dim i as int
		dim j as int
		for i=0;i<node_num;i++
			nodes[i].CreateThread(init_SGD,0)
		end
		barrier.Enter(-1)

		for i=0;i<node_num;i++
			for j=0;j<thread_per_node;j++
				nodes[i].CreateThread(do_SGD,i*node_num+j)
			end
		end
		barrier.Enter(-1)
	end
	
	public override function predict(datapoint as double[], offset as int) as double
		
	end
	public override function evaluate(mdataset as double[] global,offset as int) as double

	end

	public override constructor initialize(slave_model as SGDOptimizer)
		s_barrier=new RBarrier(node_num+1)
		s_barrier_main=new RBarrier(node_num*thread_per_node)
		sgd_model=slave_model
	end
end
